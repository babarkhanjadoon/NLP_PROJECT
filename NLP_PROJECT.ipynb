{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e611239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lines=pd.read_csv(r\"English_to_Roman_Urdu - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0dd847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>urdu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how is things?</td>\n",
       "      <td>cheezein kesi hain?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how is life?</td>\n",
       "      <td>zindagi kesi chal rahi?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is happening?</td>\n",
       "      <td>kiya horaha&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is going on?</td>\n",
       "      <td>kiya chalraha?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hey there!</td>\n",
       "      <td>arey haan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>what is up?</td>\n",
       "      <td>aur kiya horaha hai?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good mornng</td>\n",
       "      <td>subah bakhair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good day</td>\n",
       "      <td>acha din</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>good afternoon</td>\n",
       "      <td>dopehr bakahir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>good evening</td>\n",
       "      <td>sham bakhair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>good night</td>\n",
       "      <td>shab bakhair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pleased to meet you</td>\n",
       "      <td>app se mil kar nihayat khushi hwi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>it is a pleasure to meet you</td>\n",
       "      <td>app se mil kar acha laga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>how do you do?</td>\n",
       "      <td>app kese hain?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>how are you doing?</td>\n",
       "      <td>app kese hain?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>how have you been?</td>\n",
       "      <td>app kese rahe?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it is nice to meet you</td>\n",
       "      <td>app se mil kar acha laga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>how is it going?</td>\n",
       "      <td>kesa chal raha hai?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>how are you going?</td>\n",
       "      <td>app kese ja rahe hain?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nice to see you</td>\n",
       "      <td>app ko dekh ke acha laga</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             eng                               urdu\n",
       "0                 how is things?                cheezein kesi hain?\n",
       "1                   how is life?            zindagi kesi chal rahi?\n",
       "2             what is happening?                       kiya horaha>\n",
       "3              what is going on?                     kiya chalraha?\n",
       "4                     hey there!                          arey haan\n",
       "5                    what is up?               aur kiya horaha hai?\n",
       "6                    good mornng                      subah bakhair\n",
       "7                       good day                           acha din\n",
       "8                 good afternoon                     dopehr bakahir\n",
       "9                   good evening                       sham bakhair\n",
       "10                    good night                       shab bakhair\n",
       "11           pleased to meet you  app se mil kar nihayat khushi hwi\n",
       "12  it is a pleasure to meet you           app se mil kar acha laga\n",
       "13                how do you do?                     app kese hain?\n",
       "14            how are you doing?                     app kese hain?\n",
       "15            how have you been?                     app kese rahe?\n",
       "16        it is nice to meet you           app se mil kar acha laga\n",
       "17              how is it going?                kesa chal raha hai?\n",
       "18            how are you going?             app kese ja rahe hain?\n",
       "19               nice to see you           app ko dekh ke acha laga"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b368134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22aa5362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>urdu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>have fun</td>\n",
       "      <td>START_ mazey karo _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>how is your day going</td>\n",
       "      <td>START_ app ka din kesa chal raha _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>sir how are you</td>\n",
       "      <td>START_ janab aap kesy han _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>hope you feel better</td>\n",
       "      <td>START_ umeed hai keh app behtar mehsoos karein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>its good to see you again</td>\n",
       "      <td>START_ tumhe dobara dekh ke acha laga _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>how is your day</td>\n",
       "      <td>START_ app ka din kesa hai _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>you are so humble</td>\n",
       "      <td>START_ tum bht aajiz hou _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>whats up</td>\n",
       "      <td>START_ kiya chal raha hai _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is going on</td>\n",
       "      <td>START_ kiya chalraha _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>do you have some place</td>\n",
       "      <td>START_ kiya aap ke paas koi jagah hai _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           eng  \\\n",
       "65                    have fun   \n",
       "36       how is your day going   \n",
       "107            sir how are you   \n",
       "63        hope you feel better   \n",
       "53   its good to see you again   \n",
       "35             how is your day   \n",
       "102          you are so humble   \n",
       "39                    whats up   \n",
       "3             what is going on   \n",
       "100     do you have some place   \n",
       "\n",
       "                                                  urdu  \n",
       "65                              START_ mazey karo _END  \n",
       "36               START_ app ka din kesa chal raha _END  \n",
       "107                     START_ janab aap kesy han _END  \n",
       "63   START_ umeed hai keh app behtar mehsoos karein...  \n",
       "53          START_ tumhe dobara dekh ke acha laga _END  \n",
       "35                     START_ app ka din kesa hai _END  \n",
       "102                      START_ tum bht aajiz hou _END  \n",
       "39                      START_ kiya chal raha hai _END  \n",
       "3                            START_ kiya chalraha _END  \n",
       "100         START_ kiya aap ke paas koi jagah hai _END  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercase all characters\n",
    "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "lines.urdu=lines.urdu.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove quotes\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines.urdu=lines.urdu.apply(lambda x: re.sub(\"'\", '', x))\n",
    "\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "\n",
    "# Remove all the special characters\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.urdu=lines.urdu.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Remove all numbers from text\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines.eng=lines.eng.apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "# Remove extra spaces\n",
    "lines.eng=lines.eng.apply(lambda x: x.strip())\n",
    "lines.urdu=lines.urdu.apply(lambda x: x.strip())\n",
    "lines.eng=lines.eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "lines.urdu=lines.urdu.apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "# Add start and end tokens to target sequences\n",
    "lines.urdu = lines.urdu.apply(lambda x : 'START_ '+ x + ' _END')\n",
    "\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2c3367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Source Length: 9\n",
      "Max Target Lenght: 11\n",
      "Max Source Length: 9\n",
      "Max Target Lenght: 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vocabulary of English\n",
    "all_eng_words=set()# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "print('Max Source Length:',max_length_src)\n",
    "\n",
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in lines.urdu:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "print('Max Target Lenght:',max_length_tar)\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "\n",
    "# Vocabulary of Urdu \n",
    "all_urdu_words=set()\n",
    "for urdu in lines.urdu:\n",
    "    for word in urdu.split():\n",
    "        if word not in all_urdu_words:\n",
    "            all_urdu_words.add(word)# Max Length of source sequence\n",
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_src = np.max(lenght_list)\n",
    "print('Max Source Length:',max_length_src)\n",
    "\n",
    "# Max Length of target sequence\n",
    "lenght_list=[]\n",
    "for l in lines.urdu:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "max_length_tar = np.max(lenght_list)\n",
    "print('Max Target Lenght:',max_length_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8962243b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 153)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_urdu_words))\n",
    "num_encoder_tokens = len(all_eng_words)+1\n",
    "num_decoder_tokens = len(all_urdu_words)+1\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8409c0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens += 1 # For zero padding\n",
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0012af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>urdu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>good night</td>\n",
       "      <td>START_ shab bakhair _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>what is new</td>\n",
       "      <td>START_ naya kiya hai _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>happy to meet you</td>\n",
       "      <td>START_ aap se mil ke khushi hui _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>its been a while</td>\n",
       "      <td>START_ kaafi dair hwi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>nice to meet you</td>\n",
       "      <td>START_ app se mil ke acha laga _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>how are you</td>\n",
       "      <td>START_ kese ho tum _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>good morning</td>\n",
       "      <td>START_ salaam alaikum _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>good luck</td>\n",
       "      <td>START_ achi qismat _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good day</td>\n",
       "      <td>START_ acha din _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>how have you been</td>\n",
       "      <td>START_ app kese rahe _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  eng                                  urdu\n",
       "10         good night              START_ shab bakhair _END\n",
       "29        what is new             START_ naya kiya hai _END\n",
       "97  happy to meet you  START_ aap se mil ke khushi hui _END\n",
       "49   its been a while            START_ kaafi dair hwi _END\n",
       "74   nice to meet you   START_ app se mil ke acha laga _END\n",
       "78        how are you               START_ kese ho tum _END\n",
       "80       good morning            START_ salaam alaikum _END\n",
       "47          good luck               START_ achi qismat _END\n",
       "7            good day                  START_ acha din _END\n",
       "15  how have you been             START_ app kese rahe _END"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "\n",
    "lines = shuffle(lines)\n",
    "lines.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeb512bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99,), (12,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train - Test Split\n",
    "X, y = lines.eng, lines.urdu\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "729b1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the train and test dataframes for reproducing the results later, as they are shuffled.\n",
    "\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79a9d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X = X_train, y = y_train, batch_size = 64):\n",
    "    ''' Generate a batch of data '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
    "                    if t>0:\n",
    "                        # decoder target sequence (one hot encoded)\n",
    "                        # does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b57f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "latent_dim = 256\n",
    "print(train_samples//batch_size)\n",
    "print(val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37353e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26bcfc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8bd22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "504cdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2720069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "99/99 [==============================] - 16s 63ms/step - loss: 4.3253 - acc: 0.1968 - val_loss: 4.1098 - val_acc: 0.2745\n",
      "Epoch 2/50\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 3.8596 - acc: 0.2209 - val_loss: 3.7063 - val_acc: 0.2941\n",
      "Epoch 3/50\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 3.5216 - acc: 0.2430 - val_loss: 3.6326 - val_acc: 0.3137\n",
      "Epoch 4/50\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 3.2105 - acc: 0.2831 - val_loss: 3.4806 - val_acc: 0.3333\n",
      "Epoch 5/50\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 2.9338 - acc: 0.3273 - val_loss: 3.2646 - val_acc: 0.4510\n",
      "Epoch 6/50\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 2.7196 - acc: 0.3594 - val_loss: 3.2450 - val_acc: 0.4510\n",
      "Epoch 7/50\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 2.5202 - acc: 0.4016 - val_loss: 3.1999 - val_acc: 0.4314\n",
      "Epoch 8/50\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 2.3318 - acc: 0.4378 - val_loss: 3.1631 - val_acc: 0.4118\n",
      "Epoch 9/50\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 2.1558 - acc: 0.4759 - val_loss: 3.1489 - val_acc: 0.4510\n",
      "Epoch 10/50\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 1.9812 - acc: 0.5000 - val_loss: 2.9588 - val_acc: 0.4706\n",
      "Epoch 11/50\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 1.8124 - acc: 0.5422 - val_loss: 2.9062 - val_acc: 0.5098\n",
      "Epoch 12/50\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 1.6426 - acc: 0.5803 - val_loss: 2.8391 - val_acc: 0.5294\n",
      "Epoch 13/50\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 1.4921 - acc: 0.6165 - val_loss: 2.8457 - val_acc: 0.5686\n",
      "Epoch 14/50\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 1.3552 - acc: 0.6426 - val_loss: 2.8186 - val_acc: 0.5686\n",
      "Epoch 15/50\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 1.2258 - acc: 0.6867 - val_loss: 2.9329 - val_acc: 0.5490\n",
      "Epoch 16/50\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 1.1000 - acc: 0.7209 - val_loss: 2.7805 - val_acc: 0.5490\n",
      "Epoch 17/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.9959 - acc: 0.7550 - val_loss: 2.7847 - val_acc: 0.5294\n",
      "Epoch 18/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.8774 - acc: 0.7912 - val_loss: 2.7949 - val_acc: 0.5490\n",
      "Epoch 19/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.7783 - acc: 0.8213 - val_loss: 2.7341 - val_acc: 0.5294\n",
      "Epoch 20/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.6741 - acc: 0.8474 - val_loss: 2.7633 - val_acc: 0.5490\n",
      "Epoch 21/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.5997 - acc: 0.8775 - val_loss: 2.7259 - val_acc: 0.5686\n",
      "Epoch 22/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.5174 - acc: 0.8835 - val_loss: 2.8421 - val_acc: 0.5490\n",
      "Epoch 23/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.4397 - acc: 0.9177 - val_loss: 2.7818 - val_acc: 0.5686\n",
      "Epoch 24/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.3796 - acc: 0.9237 - val_loss: 2.8514 - val_acc: 0.5098\n",
      "Epoch 25/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.3224 - acc: 0.9478 - val_loss: 2.9435 - val_acc: 0.5490\n",
      "Epoch 26/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.2826 - acc: 0.9478 - val_loss: 2.9799 - val_acc: 0.5490\n",
      "Epoch 27/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.2446 - acc: 0.9598 - val_loss: 3.0130 - val_acc: 0.5294\n",
      "Epoch 28/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.2294 - acc: 0.9578 - val_loss: 3.0065 - val_acc: 0.5294\n",
      "Epoch 29/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.1843 - acc: 0.9719 - val_loss: 3.0299 - val_acc: 0.5490\n",
      "Epoch 30/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.1707 - acc: 0.9739 - val_loss: 3.0813 - val_acc: 0.5294\n",
      "Epoch 31/50\n",
      "99/99 [==============================] - 4s 38ms/step - loss: 0.1515 - acc: 0.9759 - val_loss: 3.1001 - val_acc: 0.5294\n",
      "Epoch 32/50\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 0.1476 - acc: 0.9659 - val_loss: 3.2858 - val_acc: 0.5098\n",
      "Epoch 33/50\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 0.1415 - acc: 0.9679 - val_loss: 3.2262 - val_acc: 0.5098\n",
      "Epoch 34/50\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 0.1266 - acc: 0.9739 - val_loss: 3.2710 - val_acc: 0.5098\n",
      "Epoch 35/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.1187 - acc: 0.9759 - val_loss: 3.3268 - val_acc: 0.5294\n",
      "Epoch 36/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.1102 - acc: 0.9799 - val_loss: 3.3715 - val_acc: 0.5098\n",
      "Epoch 37/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.1033 - acc: 0.9819 - val_loss: 3.3839 - val_acc: 0.5098\n",
      "Epoch 38/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0984 - acc: 0.9779 - val_loss: 3.4454 - val_acc: 0.5098\n",
      "Epoch 39/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0959 - acc: 0.9779 - val_loss: 3.4708 - val_acc: 0.5098\n",
      "Epoch 40/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0932 - acc: 0.9779 - val_loss: 3.5246 - val_acc: 0.5098\n",
      "Epoch 41/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0903 - acc: 0.9779 - val_loss: 3.5630 - val_acc: 0.5098\n",
      "Epoch 42/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0876 - acc: 0.9779 - val_loss: 3.6144 - val_acc: 0.5098\n",
      "Epoch 43/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0859 - acc: 0.9779 - val_loss: 3.6206 - val_acc: 0.5098\n",
      "Epoch 44/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0842 - acc: 0.9779 - val_loss: 3.6651 - val_acc: 0.5098\n",
      "Epoch 45/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0829 - acc: 0.9779 - val_loss: 3.6806 - val_acc: 0.5294\n",
      "Epoch 46/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0816 - acc: 0.9779 - val_loss: 3.7028 - val_acc: 0.5098\n",
      "Epoch 47/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0808 - acc: 0.9779 - val_loss: 3.7104 - val_acc: 0.5294\n",
      "Epoch 48/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0800 - acc: 0.9779 - val_loss: 3.7317 - val_acc: 0.5098\n",
      "Epoch 49/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0795 - acc: 0.9779 - val_loss: 3.7419 - val_acc: 0.5294\n",
      "Epoch 50/50\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0790 - acc: 0.9779 - val_loss: 3.7565 - val_acc: 0.5294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1948af040>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(generate_batch(X_train, y_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
    "                    validation_steps = val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03f1196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Always remember to save the weights\n",
    "\n",
    "model.save_weights('etu_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f7c7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights, if you close the application\n",
    "\n",
    "model.load_weights('etu_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b539c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference Setup\n",
    "\n",
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07679d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode sample sequeces\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea36e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ba5f82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Input English sentence: how are you going\n",
      "Actual Urdu Translation:  app kese ja rahe hain \n",
      "Predicted Urdu Translation:  app kese ja rahe hain \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f21e9f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['app', 'kese', 'ja', 'rahe', 'hain']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [\n",
    "    y_train[k:k+1].values[0][6:-4].split()\n",
    "]\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7c1f005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score -> 1.0\n"
     ]
    }
   ],
   "source": [
    "candidate = decoded_sentence[:-4].split()\n",
    "print('BLEU score -> {}'.format(sentence_bleu(reference, candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91f32316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Input English sentence: whats up\n",
      "Actual Urdu Translation:  kiya chal raha hai \n",
      "Predicted Urdu Translation:  kiya chal raha hai \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eb0cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kiya', 'chal', 'raha', 'hai']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [\n",
    "    y_train[k:k+1].values[0][6:-4].split()\n",
    "]\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19a930cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score -> 1.0\n"
     ]
    }
   ],
   "source": [
    "candidate = decoded_sentence[:-4].split()\n",
    "print('BLEU score -> {}'.format(sentence_bleu(reference, candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f10f3e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Input English sentence: good night\n",
      "Actual Urdu Translation:  shab bakhair \n",
      "Predicted Urdu Translation:  shab bakhair \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(train_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_train[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47c6ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['shab', 'bakhair']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [\n",
    "    y_train[k:k+1].values[0][6:-4].split()\n",
    "]\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56161fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score -> 1.491668146240062e-154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "E:\\anaconda\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "candidate = decoded_sentence[:-4].split()\n",
    "print('BLEU score -> {}'.format(sentence_bleu(reference, candidate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56651792",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = generate_batch(X_test, y_test, batch_size = 1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f9fce066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Input English sentence: i would love to travel again\n",
      "Actual Urdu Translation:  mein dobara safar krna pasand krun ga \n",
      "Predicted Urdu Translation:  mai apko dobara dekh ke khush hun \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e174f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Input English sentence: good evening\n",
      "Actual Urdu Translation:  shaam bakhair \n",
      "Predicted Urdu Translation:  sham bakhair \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2a74bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Input English sentence: hi there\n",
      "Actual Urdu Translation:  hi haan \n",
      "Predicted Urdu Translation:  hi \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq, actual_output), _ = next(val_gen)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('Input English sentence:', X_test[k:k+1].values[0])\n",
    "print('Actual Urdu Translation:', y_test[k:k+1].values[0][6:-4])\n",
    "print('Predicted Urdu Translation:', decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1be12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
